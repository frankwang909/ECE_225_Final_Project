{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\"\"\"\n@author: nakayama.s\n\"\"\"\n\nimport os\nimport warnings\nimport gc\nimport time\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b2c3d3bb82c68239773a6e79e617f9b09a7e1f2"},"cell_type":"code","source":"def graph_insight(data):\n    print(set(data.dtypes.tolist()))\n    df_num = data.select_dtypes(include = ['float64', 'int64'])\n    df_num.hist(figsize=(16, 16), bins=50, xlabelsize=8, ylabelsize=8);\n\ndef eda(data):\n    # print(data)\n    print(\"----------Top-5- Record----------\")\n    print(data.head(5))\n    print(\"-----------Information-----------\")\n    print(data.info())\n    print(\"-----------Data Types------------\")\n    print(data.dtypes)\n    print(\"----------Missing value----------\")\n    print(data.isnull().sum())\n    print(\"----------Null value-------------\")\n    print(data.isna().sum())\n    print(\"----------Shape of Data----------\")\n    print(data.shape)\n    print(\"----------describe---------------\")\n    print(data.describe())\n    print(\"----------tail-------------------\")\n    print(data.tail())\n    \ndef read_csv(path):\n  # logger.debug('enter')\n  df = pd.read_csv(path)\n  # logger.debug('exit')\n  return df\n\ndef load_train_data():\n  # logger.debug('enter')\n  df = read_csv(SALES_TRAIN_V2)\n  # logger.debug('exit')\n  return df\n\ndef load_test_data():\n  # logger.debug('enter')\n  df = read_csv(TEST_DATA)\n  # logger.debug('exit')\n  return df\n\ndef graph_insight(data):\n    print(set(data.dtypes.tolist()))\n    df_num = data.select_dtypes(include = ['float64', 'int64'])\n    df_num.hist(figsize=(16, 16), bins=50, xlabelsize=8, ylabelsize=8);\n\ndef drop_duplicate(data, subset):\n    print('Before drop shape:', data.shape)\n    before = data.shape[0]\n    data.drop_duplicates(subset,keep='first', inplace=True) #subset is list where you have to put all column for duplicate check\n    data.reset_index(drop=True, inplace=True)\n    print('After drop shape:', data.shape)\n    after = data.shape[0]\n    print('Total Duplicate:', before-after)\n\ndef unresanable_data(data):\n    print(\"Min Value:\",data.min())\n    print(\"Max Value:\",data.max())\n    print(\"Average Value:\",data.mean())\n    print(\"Center Point of Data:\",data.median())\n\nSAMPLE_SUBMISSION    = '../input/sample_submission_V2.csv'\nTRAIN_DATA           = '../input/train_V2.csv'\nTEST_DATA            = '../input/test_V2.csv'\n\nsample       = read_csv(SAMPLE_SUBMISSION)\ntrain           = read_csv(TRAIN_DATA)\ntest            = read_csv(TEST_DATA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbb5c6f58e0c3acd20c31206ace18279af33f25b"},"cell_type":"code","source":"eda(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13721cc02049f2dbf61d89d53727fde758020d3e","scrolled":false},"cell_type":"code","source":"eda(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b921523ec4e706c692d50926700cb5d91c1a66ef"},"cell_type":"code","source":"train = pd.get_dummies(train,columns=['matchType'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8beb3e0da01f3dd7705d496e948b726856a0bb74"},"cell_type":"code","source":"train =train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f7416966f91cf9f02c4146f563d46207234f3d4"},"cell_type":"code","source":"test = pd.get_dummies(test,columns=['matchType'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0bc59046becef75371c00acb8c7c6b57c66a81f"},"cell_type":"code","source":"y_train =train['winPlacePerc']\nx_train =train.drop(['Id','groupId','matchId','winPlacePerc'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae5678cee66d0c650f0b9916ed5fbb336356d77"},"cell_type":"code","source":"X_train = x_train.values\nY_train = y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13fbd37704b557d94ed77bfe6329e17106db8540"},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"313a0d832e3e3f62f4c113bd5f1c852f989a4a84"},"cell_type":"code","source":"Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"330c9db41e92da409a487991fad44e743b273583"},"cell_type":"code","source":"from keras import models\nfrom keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout, Input\n\ndef build_model():\n    # Because we will need to instantiate\n    # the same model multiple times,\n    # we use a function to construct it.\n    model = Sequential()\n    model.add(Dense(80,input_dim=X_train.shape[1],activation='relu'))\n    model.add(Dense(160,activation='relu'))\n    model.add(Dense(320,activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(160,activation='relu'))\n    model.add(Dense(80,activation='relu'))\n    model.add(Dense(40,activation='relu'))\n    model.add(Dense(20,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n#     model.summary\n#     history = model.fit(X_train, y_train, epochs=70,batch_size=100000)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"432a51b870c2be768f4b11cb14bed06245ef0e6c"},"cell_type":"code","source":"import numpy as np\n\nk = 4\nnum_val_samples = len(X_train) // k\n# num_epochs = 40\n# all_scores = []\n# for i in tqdm(range(k)):\n#     print('processing fold #', i)\n#     # Prepare the validation data: data from partition # k\n#     val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n#     val_targets = Y_train[i * num_val_samples: (i + 1) * num_val_samples]\n\n#     # Prepare the training data: data from all other partitions\n#     partial_train_data = np.concatenate([X_train[:i * num_val_samples],X_train[(i + 1) * num_val_samples:]],axis=0)\n#     partial_train_targets = np.concatenate([Y_train[:i * num_val_samples],Y_train[(i + 1) * num_val_samples:]],axis=0)\n\n#     # Build the Keras model (already compiled)\n#     model = build_model()\n#     # Train the model (in silent mode, verbose=0)\n#     model.fit(partial_train_data, partial_train_targets,\n#               epochs=num_epochs, batch_size=100000, verbose=2)\n#     # Evaluate the model on the validation data\n#     val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=2)\n#     all_scores.append(val_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c06b03c06198db07c468072cefbacc81b5e1b91"},"cell_type":"code","source":"# all_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10378dc49f5831b82d1ef03760d61785e6bc9752"},"cell_type":"code","source":"# np.mean(all_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b1a79c0ca37c27203ed2b144bd873965635e74d"},"cell_type":"code","source":"from keras import backend as K\n\n# Some memory clean-up\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ef8abfdca04251d4b2745e1d11178b0571ad673"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping \n \n# Early-stopping \n# early_stopping = EarlyStopping(patience=0, monitor='val_mean_absolute_error',verbose=2,) \n# callbacks=[early_stopping]\n\nnum_epochs = 500\nall_mae_histories = []\nfor i in tqdm(range(k)):\n    print('processing fold #', i)\n    # Prepare the validation data: data from partition # k\n    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = Y_train[i * num_val_samples: (i + 1) * num_val_samples]\n\n    # Prepare the training data: data from all other partitions\n    partial_train_data = np.concatenate([X_train[:i * num_val_samples],X_train[(i + 1) * num_val_samples:]],axis=0)\n    partial_train_targets = np.concatenate([Y_train[:i * num_val_samples],Y_train[(i + 1) * num_val_samples:]],axis=0)\n\n    # Build the Keras model (already compiled)\n    model = build_model()\n    # Train the model (in silent mode, verbose=0)\n    history = model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets),epochs=num_epochs, batch_size=10000, verbose=2)\n    mae_history = history.history['val_mean_absolute_error']\n    all_mae_histories.append(mae_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc2465c494fe3a495ffad09b758409a180efa8d"},"cell_type":"code","source":"average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55b79d8e40abf4186398bc2530161c27dc323b4d"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"477477477651241c5e5615a62e73047ab6ae9abb"},"cell_type":"code","source":"def smooth_curve(points, factor=0.9):\n  smoothed_points = []\n  for point in tqdm(points):\n    if smoothed_points:\n      previous = smoothed_points[-1]\n      smoothed_points.append(previous * factor + point * (1 - factor))\n    else:\n      smoothed_points.append(point)\n  return smoothed_points\n\nsmooth_mae_history = smooth_curve(average_mae_history[10:])\n\nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f9d6ef8301090decd4fe8e2ecf71a93b430a4ea"},"cell_type":"code","source":"# history.history['val_mean_absolute_error']\n# history.history['val_loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b012a933aba7ac5f7b1401a4094b76c8326066f"},"cell_type":"code","source":"def plot_history(history):\n    # print(history.history.keys())\n\n    # 精度の履歴をプロット\n    plt.plot(history.history['mean_absolute_error'])\n    plt.plot(history.history['val_mean_absolute_error'])\n    plt.title('model accuracy')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.legend(['mean_absolute_error', 'val_mean_absolute_error'], loc='lower right')\n    plt.show()\n\n    # 損失の履歴をプロット\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['loss', 'val_loss'], loc='lower right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8794ff2451e5ca20a5ce2da472fc9fbcf769d708"},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac0a2c747ea913745db66b63c30df231bac187ea"},"cell_type":"code","source":"x_test = test.drop(['Id','groupId','matchId'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95a9cdcb13815670ff0c82df9301fa3d5d656e19"},"cell_type":"code","source":"X_test = x_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc2665393e0f9b13b14048a9705097083b2c771f"},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6cbc3a34060ee621a8024216fad342d43096d42"},"cell_type":"code","source":"model = build_model()\n# Train it on the entirety of the data.\nmodel.fit(train_data, train_targets,epochs=70, batch_size=16, verbose=1)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0da247d92ae63797f37851b2f21512d4e4008b51"},"cell_type":"code","source":"prediction = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9322f746f10a9406c510fc5b4c5c70e19c0c6af6"},"cell_type":"code","source":"sample['winPlacePerc'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d51931a2006f29c17a144cb1147b424d0b312e4c"},"cell_type":"code","source":"sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fee020d25a737daf4d780c613c9e4944fca978c"},"cell_type":"code","source":"sample.to_csv('sample_submission_v1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}